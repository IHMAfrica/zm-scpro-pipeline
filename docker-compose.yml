name: carepro-pipeline

services:
  # -------------------------
  # ZOOKEEPER
  # -------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.3
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-lc", "cub zk-ready localhost:2181 30 5"]
      interval: 10s
      timeout: 5s
      retries: 30

  # -------------------------
  # KAFKA (dual listeners)
  # -------------------------
  kafka:
    image: confluentinc/cp-kafka:7.3.3
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"   # PLAINTEXT
      - "9093:9093"   # SASL_PLAINTEXT
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # dual listeners exposed; choose via .env KAFKA_BOOTSTRAP=PLAINTEXT://kafka:9092 or SASL_PLAINTEXT://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,SASL_PLAINTEXT://kafka:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    healthcheck:
      test: ["CMD", "bash", "-lc", "cub kafka-ready -b localhost:9092 1 20 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        # Create SCRAM user only if SASL vars are present
        if [[ -n "${KAFKA_SASL_USER:-}" && -n "${KAFKA_SASL_PASSWORD:-}" ]]; then
          if ! kafka-configs --zookeeper zookeeper:2181 --describe --entity-type users --entity-name "${KAFKA_SASL_USER}" >/dev/null 2>&1; then
            kafka-configs --zookeeper zookeeper:2181 --alter \
              --add-config "SCRAM-SHA-512=[iterations=8192,password=${KAFKA_SASL_PASSWORD}]" \
              --entity-type users --entity-name "${KAFKA_SASL_USER}";
          fi
        fi
        exec /etc/confluent/docker/run

  # -------------------------
  # SCHEMA REGISTRY
  # -------------------------
  schema-registry:
    image: confluentinc/cp-schema-registry:7.3.3
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    # Avoid 8081 clash with Flink UI
    ports:
      - "8082:8081"
    environment:
      # Bootstrap with scheme (matches Confluent expectations)
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      # Security (defaults for local dev = PLAINTEXT)
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: ${SCHEMA_REGISTRY_SECURITY_PROTOCOL:-PLAINTEXT}
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: ${SCHEMA_REGISTRY_SASL_MECHANISM:-}
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: ${SCHEMA_REGISTRY_SASL_JAAS_CONFIG:-}

  # -------------------------
  # DEBEZIUM CONNECT
  # -------------------------
  connect:
    image: debezium/connect:2.4
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_started
    ports:
      - "8083:8083"
    environment:
      # Connect needs raw host:port, not scheme
      BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: _connect-configs
      OFFSET_STORAGE_TOPIC: _connect-offsets
      STATUS_STORAGE_TOPIC: _connect-status
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      # Security (optional in dev)
      CONNECT_SECURITY_PROTOCOL: ${CONNECT_SECURITY_PROTOCOL:-PLAINTEXT}
      CONNECT_SASL_MECHANISM: ${CONNECT_SASL_MECHANISM:-}
      CONNECT_SASL_JAAS_CONFIG: ${CONNECT_SASL_JAAS_CONFIG:-}
    volumes:
      - ./connectors:/connectors:ro

  # -------------------------
  # POSTGRES (sink)
  # -------------------------
  postgres:
    image: postgres:14
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./postgres/init:/docker-entrypoint-initdb.d:ro

  # -------------------------
  # FLINK
  # -------------------------
  flink-jobmanager:
    build:
      context: .
      dockerfile: docker/flink/Dockerfile
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_started
      postgres:
        condition: service_started
    ports:
      - "8081:8081"   # Flink UI
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      # Kafka (dynamic via .env)
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER:-}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME:-}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD:-}
      # Schema Registry
      SCHEMA_REGISTRY: http://schema-registry:8081
      # Postgres
      POSTGRES_HOST: postgres
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      # Flink tuning
      FLINK_PARALLELISM: ${FLINK_PARALLELISM:-2}
      FLINK_CHECKPOINT_INTERVAL_MS: ${FLINK_CHECKPOINT_INTERVAL_MS:-60000}
    command: ["/bin/bash","-lc","/docker-entrypoint.sh jobmanager"]
    volumes:
      - ./flink/job:/opt/flink/usrlib/job:ro

  flink-taskmanager:
    build:
      context: .
      dockerfile: docker/flink/Dockerfile
    restart: unless-stopped
    depends_on:
      flink-jobmanager:
        condition: service_started
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      FLINK_PARALLELISM: ${FLINK_PARALLELISM:-2}
    command: ["/bin/bash","-lc","/docker-entrypoint.sh taskmanager"]
    volumes:
      - ./flink/job:/opt/flink/usrlib/job:ro

  # -------------------------
  # PRODUCERS (Python)
  # -------------------------
  producer-art:
    image: python:3.10-slim
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    working_dir: /app
    environment:
      # SQL Server
      SQLSERVER_HOST: ${SQLSERVER_HOST}
      SQLSERVER_PORT: ${SQLSERVER_PORT}
      SQLSERVER_DB: ${SQLSERVER_DB}
      SQLSERVER_USER: ${SQLSERVER_USER}
      SQLSERVER_PASSWORD: ${SQLSERVER_PASSWORD}
      SQLSERVER_TDS_VERSION: ${SQLSERVER_TDS_VERSION:-7.3}
      # Kafka (code expects both forms)
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER:-}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME:-}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD:-}
      KAFKA_CLIENT_ID: ${KAFKA_CLIENT_ID:-carepro-producer}
      KAFKA_COMPRESSION_TYPE: ${KAFKA_COMPRESSION_TYPE:-snappy}
      KAFKA_TOPIC_ART: ${KAFKA_TOPIC_ART}
      BATCH_SIZE: ${BATCH_SIZE:-5000}
      SQL_ART_QUERY_FILE: /app/sql/art_cohort.sql
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        apt-get update && apt-get install -y --no-install-recommends netcat-openbsd && rm -rf /var/lib/apt/lists/*
        pip install --no-cache-dir pymssql==2.2.8 confluent-kafka==2.5.0
        HOSTPORT="$(printf '%s' "${KAFKA_BOOTSTRAP}" | sed -E 's#^[^:]+://##')"   # strip scheme for socket probe
        echo "Waiting for ${HOSTPORT} (timeout 180s)..."
        bash /app/scripts/wait-for.sh "${HOSTPORT}" 180
        python /app/producer_art.py
    volumes:
      - ./producers:/app:ro
      - ./sql:/app/sql:ro
      - ./scripts:/app/scripts:ro

  producer-interactions:
    image: python:3.10-slim
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    working_dir: /app
    environment:
      # SQL Server
      SQLSERVER_HOST: ${SQLSERVER_HOST}
      SQLSERVER_PORT: ${SQLSERVER_PORT}
      SQLSERVER_DB: ${SQLSERVER_DB}
      SQLSERVER_USER: ${SQLSERVER_USER}
      SQLSERVER_PASSWORD: ${SQLSERVER_PASSWORD}
      SQLSERVER_TDS_VERSION: ${SQLSERVER_TDS_VERSION:-7.3}
      # Kafka
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER:-}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME:-}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD:-}
      KAFKA_CLIENT_ID: ${KAFKA_CLIENT_ID:-carepro-producer}
      KAFKA_COMPRESSION_TYPE: ${KAFKA_COMPRESSION_TYPE:-snappy}
      KAFKA_TOPIC_INTERACTIONS: ${KAFKA_TOPIC_INTERACTIONS}
      BATCH_SIZE: ${BATCH_SIZE:-5000}
      SQL_INTERACTIONS_QUERY_FILE: /app/sql/carepro_all_interactions.sql
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        apt-get update && apt-get install -y --no-install-recommends netcat-openbsd && rm -rf /var/lib/apt/lists/*
        pip install --no-cache-dir pymssql==2.2.8 confluent-kafka==2.5.0
        HOSTPORT="$(printf '%s' "${KAFKA_BOOTSTRAP}" | sed -E 's#^[^:]+://##')"
        echo "Waiting for ${HOSTPORT} (timeout 180s)..."
        bash /app/scripts/wait-for.sh "${HOSTPORT}" 180
        python /app/producer_interactions.py
    volumes:
      - ./producers:/app:ro
      - ./sql:/app/sql:ro
      - ./scripts:/app/scripts:ro

  # -------------------------
  # TOPIC BOOTSTRAP
  # -------------------------
  bootstrap:
    image: bash:5.2
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["bash","/connectors/topics.sh"]
    environment:
      # Provide both; your script can choose
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL:-PLAINTEXT}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER:-}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME:-}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD:-}
    volumes:
      - ./connectors:/connectors:ro
