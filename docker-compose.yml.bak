name: carepro-pipeline

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.3
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # allow 4lw queries; not strictly needed for this healthcheck but useful
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: "ruok,stat,mntr,conf,cons,isro"
      # keep heap small for dev machines
      KAFKA_HEAP_OPTS: "-Xms256M -Xmx256M"
    healthcheck:
      # Use zookeeper-shell (shipped in this image) instead of `cub`
      test: ["CMD-SHELL", "zookeeper-shell localhost:2181 ls / >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60

  kafka:
    image: confluentinc/cp-kafka:7.3.3
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"   # PLAINTEXT for dev
      - "9093:9093"   # SASL_PLAINTEXT (off in dev; on when you set .env)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      # Dual listeners: dev uses PLAINTEXT:9092, prod can use SASL_PLAINTEXT:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,SASL_PLAINTEXT://kafka:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Single-broker-safe settings
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

      # SCRAM knobs (idle in dev; activated when you fill creds in .env)
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"

      # Give Kafka a bit of memory headroom
      KAFKA_HEAP_OPTS: "-Xms512M -Xmx512M"
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        # Provision SCRAM user only when SASL creds are present (prod path)
        if [[ -n "${KAFKA_SASL_USER:-}" && -n "${KAFKA_SASL_PASSWORD:-}" ]]; then
          if ! kafka-configs --zookeeper zookeeper:2181 --describe --entity-type users --entity-name ${KAFKA_SASL_USER} >/dev/null 2>&1; then
            kafka-configs --zookeeper zookeeper:2181 --alter \
              --add-config "SCRAM-SHA-512=[iterations=8192,password=${KAFKA_SASL_PASSWORD}]" \
              --entity-type users --entity-name ${KAFKA_SASL_USER};
          fi
        fi
        exec /etc/confluent/docker/run
    healthcheck:
      test: ["CMD", "bash", "-lc", "cub kafka-ready -b localhost:9092 1 60 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30

  schema-registry:
    image: confluentinc/cp-schema-registry:7.3.3
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    # avoid 8081 clash with Flink JM
    ports:
      - "8082:8081"
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: ${SCHEMA_REGISTRY_SECURITY_PROTOCOL}
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: ${SCHEMA_REGISTRY_SASL_MECHANISM}
      SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG: ${SCHEMA_REGISTRY_SASL_JAAS_CONFIG}

  connect:
    image: debezium/connect:2.4
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_started
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP}
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: _connect-configs
      OFFSET_STORAGE_TOPIC: _connect-offsets
      STATUS_STORAGE_TOPIC: _connect-status
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_SECURITY_PROTOCOL: ${CONNECT_SECURITY_PROTOCOL}
      CONNECT_SASL_MECHANISM: ${CONNECT_SASL_MECHANISM}
      CONNECT_SASL_JAAS_CONFIG: ${CONNECT_SASL_JAAS_CONFIG}

  postgres:
    image: postgres:14
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./postgres/init:/docker-entrypoint-initdb.d

  flink-jobmanager:
    build:
      context: .
      dockerfile: docker/flink/Dockerfile
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_started
      postgres:
        condition: service_started
    ports:
      - "8081:8081"
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD}
      SCHEMA_REGISTRY: http://schema-registry:8081
      POSTGRES_HOST: postgres
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      FLINK_PARALLELISM: ${FLINK_PARALLELISM}
      FLINK_CHECKPOINT_INTERVAL_MS: ${FLINK_CHECKPOINT_INTERVAL_MS}
    command: ["/bin/bash","-lc","/docker-entrypoint.sh jobmanager"]
    volumes:
      - ./flink/job:/opt/flink/usrlib/job:ro

  flink-taskmanager:
    build:
      context: .
      dockerfile: docker/flink/Dockerfile
    restart: unless-stopped
    depends_on:
      flink-jobmanager:
        condition: service_started
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      FLINK_PARALLELISM: ${FLINK_PARALLELISM}
    command: ["/bin/bash","-lc","/docker-entrypoint.sh taskmanager"]
    volumes:
      - ./flink/job:/opt/flink/usrlib/job:ro

  producer-art:
    image: python:3.10-slim
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    working_dir: /app
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        apt-get update && apt-get install -y --no-install-recommends netcat-openbsd && rm -rf /var/lib/apt/lists/*
        pip install --no-cache-dir pymssql==2.2.8 confluent-kafka==2.5.0
        /app/wait-for.sh kafka 9092 180
        python /app/producer_art.py
    environment:
      SQLSERVER_HOST: ${SQLSERVER_HOST}
      SQLSERVER_PORT: ${SQLSERVER_PORT}
      SQLSERVER_DB: ${SQLSERVER_DB}
      SQLSERVER_USER: ${SQLSERVER_USER}
      SQLSERVER_PASSWORD: ${SQLSERVER_PASSWORD}
      SQLSERVER_TDS_VERSION: ${SQLSERVER_TDS_VERSION}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD}
      KAFKA_CLIENT_ID: ${KAFKA_CLIENT_ID}
      KAFKA_COMPRESSION_TYPE: ${KAFKA_COMPRESSION_TYPE}
      KAFKA_TOPIC_ART: ${KAFKA_TOPIC_ART}
      BATCH_SIZE: ${BATCH_SIZE}
      SQL_ART_QUERY_FILE: /app/sql/art_cohort.sql
    volumes:
      - ./producers:/app:ro
      - ./scripts/wait-for.sh:/app/wait-for.sh:ro
      - ./sql:/app/sql:ro

  producer-interactions:
    image: python:3.10-slim
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    working_dir: /app
    command:
      - /bin/bash
      - -lc
      - |
        set -e
        apt-get update && apt-get install -y --no-install-recommends netcat-openbsd && rm -rf /var/lib/apt/lists/*
        pip install --no-cache-dir pymssql==2.2.8 confluent-kafka==2.5.0
        /app/wait-for.sh kafka 9092 180
        python /app/producer_interactions.py
    environment:
      SQLSERVER_HOST: ${SQLSERVER_HOST}
      SQLSERVER_PORT: ${SQLSERVER_PORT}
      SQLSERVER_DB: ${SQLSERVER_DB}
      SQLSERVER_USER: ${SQLSERVER_USER}
      SQLSERVER_PASSWORD: ${SQLSERVER_PASSWORD}
      SQLSERVER_TDS_VERSION: ${SQLSERVER_TDS_VERSION}
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD}
      KAFKA_CLIENT_ID: ${KAFKA_CLIENT_ID}
      KAFKA_COMPRESSION_TYPE: ${KAFKA_COMPRESSION_TYPE}
      KAFKA_TOPIC_INTERACTIONS: ${KAFKA_TOPIC_INTERACTIONS}
      BATCH_SIZE: ${BATCH_SIZE}
      SQL_INTERACTIONS_QUERY_FILE: /app/sql/carepro_all_interactions.sql
    volumes:
      - ./producers:/app:ro
      - ./scripts/wait-for.sh:/app/wait-for.sh:ro
      - ./sql:/app/sql:ro

  bootstrap:
    image: bash:5.2
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["bash","/connectors/topics.sh"]
    environment:
      KAFKA_BOOTSTRAP: ${KAFKA_BOOTSTRAP}
      KAFKA_SECURITY_PROTOCOL: ${KAFKA_SECURITY_PROTOCOL}
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER}
      KAFKA_SASL_USERNAME: ${KAFKA_SASL_USERNAME}
      KAFKA_SASL_PASSWORD: ${KAFKA_SASL_PASSWORD}
    volumes:
      - ./connectors:/connectors:ro